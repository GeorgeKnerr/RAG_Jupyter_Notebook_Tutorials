{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Documents to Embeddings: A Complete Tutorial\n",
    "\n",
    "This notebook demonstrates how to convert text documents into vector embeddings for use in Retrieval-Augmented Generation (RAG) systems. We'll explore two different embedding techniques and compare their characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Word Embeddings?\n",
    "\n",
    "**Word embeddings are numerical representations of words or text chunks.** They transform text into vectors (lists of numbers) that capture the meaning, context, and relationships between words.\n",
    "\n",
    "### Key Concepts:\n",
    "- Embeddings turn text into **vectors** that can be mathematically compared\n",
    "- Similar pieces of text have similar vector representations\n",
    "- These vectors enable **semantic search and retrieval** ‚Äî critical components in RAG systems\n",
    "\n",
    "### In Simple Words:\n",
    "**Embeddings let machines \"understand\" and \"measure\" how similar two pieces of text are ‚Äî even if they use different words.**\n",
    "\n",
    "---\n",
    "\n",
    "## Common Word Embedding Techniques\n",
    "\n",
    "There are several ways to generate embeddings. We'll explore two widely-used methods:\n",
    "\n",
    "### a) Word2Vec\n",
    "- A traditional method using shallow neural networks\n",
    "- Captures relationships between words based on how often they appear together\n",
    "- Learns embeddings like: \"king - man + woman = queen\"\n",
    "- **Custom dimension size** (e.g., 100 dimensions)\n",
    "- Best for basic word-level relationships\n",
    "\n",
    "### b) OpenAI using Langchain Embeddings\n",
    "- Uses powerful pre-trained models from OpenAI\n",
    "- The most common model: **text-embedding-ada-002**\n",
    "  - Small, fast, and very good quality\n",
    "  - Produces a **1536-dimensional vector** for each chunk\n",
    "  - Ideal for search, retrieval, and similarity tasks\n",
    "\n",
    "### How It Works:\n",
    "1. You send a chunk of text to the OpenAI API\n",
    "2. The model returns a fixed-length embedding (vector) that captures the **semantic meaning** of the text\n",
    "\n",
    "**OpenAI embeddings using Langchain are widely used in production RAG pipelines**, especially when quality and ease-of-use matter.\n",
    "\n",
    "---\n",
    "\n",
    "## Embedding Dimensions Comparison\n",
    "\n",
    "| Method | Dimension Size | Notes |\n",
    "|--------|---------------|-------|\n",
    "| Word2Vec | Custom (e.g., 100) | Based on training setup; basic context |\n",
    "| OpenAI (text-embedding-ada-002) | 1536 | Very high quality; best for production use |\n",
    "\n",
    "**All methods turn text into vectors**, but the quality and context-awareness improve as you move from Word2Vec ‚Üí OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "We've successfully extracted and chunked content from a PDF document:\n",
    "\n",
    "**\"Digital Transformation of the Healthcare Value Chain: Emergence of Medical Internet of Things (MIoT) may need an Integrated Clinical Environment, ICE Platform.\"**\n",
    "\n",
    "### Our Goals:\n",
    "1. **Generate vector representations (embeddings)** for each chunk of text\n",
    "2. **Try out two different embedding methods**: Word2Vec and OpenAI Embedding Model\n",
    "3. **Compare the differences** and prepare the data for vector-based retrieval in a RAG system\n",
    "\n",
    "These embeddings will later help us **match a user's question to the most relevant part of the document**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup: Import Required Libraries\n",
    "\n",
    "We'll need several libraries for this tutorial:\n",
    "- **numpy**: For numerical operations on vectors\n",
    "- **os**: For environment variable management\n",
    "- **openai**: For interacting with OpenAI's API\n",
    "- **dotenv**: For loading environment variables from .env files\n",
    "- **nltk**: Natural Language Toolkit for text processing\n",
    "- **gensim**: For Word2Vec model training\n",
    "- **langchain**: For document loading, text splitting, and embeddings\n",
    "- **tenacity**: For retry logic when calling APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Download NLTK Data\n",
    "\n",
    "We need NLTK's punkt tokenizer to split text into sentences. This data package helps NLTK understand sentence boundaries in different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NLTK punkt tokenizer already available\n",
      "üì• Downloading NLTK punkt_tab tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\gknerr\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NLTK punkt_tab tokenizer downloaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"‚úÖ NLTK punkt tokenizer already available\")\n",
    "except LookupError:\n",
    "    print(\"üì• Downloading NLTK punkt tokenizer...\")\n",
    "    nltk.download('punkt')\n",
    "    print(\"‚úÖ NLTK punkt tokenizer downloaded successfully\")\n",
    "\n",
    "# If you see errors about 'punkt_tab', download it as well:\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "    print(\"‚úÖ NLTK punkt_tab tokenizer already available\")\n",
    "except LookupError:\n",
    "    print(\"üì• Downloading NLTK punkt_tab tokenizer...\")\n",
    "    nltk.download('punkt_tab')\n",
    "    print(\"‚úÖ NLTK punkt_tab tokenizer downloaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Authentication: OpenAI Setup\n",
    "\n",
    "The following code sets up authentication to connect securely to OpenAI. This process:\n",
    "\n",
    "1. Retrieves the OpenAI API key from an OS environment variable (`OPENAI_API_KEY`)\n",
    "2. Initializes the OpenAI client for embedding generation\n",
    "\n",
    "This authentication enables us to:\n",
    "- Call the embedding model securely\n",
    "- Get vector embeddings for input text\n",
    "- Use the embeddings for semantic search and similarity matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI API key from environment variable\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set.\")\n",
    "\n",
    "# Set OpenAI API key for the openai library\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Set up LangChain OpenAIEmbeddings\n",
    "embeddings_client = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Define Function: Get Embeddings from OpenAI\n",
    "\n",
    "This function returns vector embeddings for the provided text chunks using OpenAI's embedding model.\n",
    "\n",
    "### How it works:\n",
    "1. Takes a list of text chunks as input\n",
    "2. Calls the OpenAI embeddings API with retry logic (using tenacity)\n",
    "3. Returns the `.data` field containing the embedding vectors\n",
    "\n",
    "### Retry Mechanism:\n",
    "The `@retry` decorator ensures that if the API call fails (due to rate limits or network issues), it will automatically retry with exponential backoff (waiting longer between each attempt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns vector embeddings for the provided text chunks.\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=45, max=120), stop=stop_after_attempt(6))\n",
    "def get_embeddings(texts_chunk):\n",
    "    return embeddings_client.embed_documents(texts_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load and Extract PDF Content\n",
    "\n",
    "We'll use **LangChain's PyMuPDFLoader** to extract text from our healthcare PDF document.\n",
    "\n",
    "### What this function does:\n",
    "1. Uses LangChain's built-in PDF loader\n",
    "2. Loads the PDF into LangChain's document format\n",
    "3. Returns a list of document chunks with their content\n",
    "\n",
    "This is the first step in our pipeline: getting raw text from the PDF before we can create embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load and extract text from PDF\n",
    "def load_pdf_with_langchain(pdf_path):\n",
    "    \n",
    "    # Use LangChain's built-in loader\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    \n",
    "    # Load the PDF into LangChain's document format\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"Successfully loaded {len(documents)} document chunks from the PDF.\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Healthcare Document\n",
    "\n",
    "Now we'll load our specific PDF about healthcare transformation and medical IoT. Update the path if your PDF is located elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 13 document chunks from the PDF.\n"
     ]
    }
   ],
   "source": [
    "# Path to the uploaded PDF (replace with your actual file path)\n",
    "pdf_path = \"./data/41598_2020_Article_64454.pdf\"\n",
    "\n",
    "# Extract the document chunks\n",
    "docs = load_pdf_with_langchain(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Approach 1: Sentence-Based Chunking for Word2Vec\n",
    "\n",
    "Word2Vec works best when trained on individual sentences. This chunking strategy:\n",
    "\n",
    "### Why sentence-based chunking?\n",
    "- **Word2Vec learns from context**: Words that appear near each other have similar meanings\n",
    "- **Sentences provide natural context boundaries**: Each sentence is a complete thought\n",
    "- **Better training data**: The model learns more accurate word relationships\n",
    "\n",
    "### How the function works:\n",
    "1. Extract page content from each document\n",
    "2. Use NLTK's `sent_tokenize` to split text into sentences\n",
    "3. Group sentences together (e.g., 3 sentences per chunk)\n",
    "4. Return a list of text chunks ready for Word2Vec training\n",
    "\n",
    "This approach is particularly useful for preparing text inputs for Word2Vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_based_chunking(docs, sentences_per_chunk=3):\n",
    "    chunks = []\n",
    "    for doc in docs:\n",
    "        sentences = sent_tokenize(doc.page_content)\n",
    "        for i in range(0, len(sentences), sentences_per_chunk):\n",
    "            chunk = \" \".join(sentences[i:i + sentences_per_chunk])\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# Generate sentence chunks\n",
    "text_chunks = sentence_based_chunking(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Generate Word2Vec Embeddings\n",
    "\n",
    "Now we'll train a Word2Vec model on our text chunks and generate embeddings.\n",
    "\n",
    "### Understanding the function:\n",
    "\n",
    "1. **Tokenize each chunk into words**: Split text into individual words\n",
    "   - Example: \"The patient needs care\" ‚Üí [\"The\", \"patient\", \"needs\", \"care\"]\n",
    "\n",
    "2. **Train a Word2Vec model** with these parameters:\n",
    "   - `sentences=tokenized`: Training data (list of word lists)\n",
    "   - `vector_size=100`: Each word becomes a 100-dimensional vector\n",
    "   - `window=5`: Look at 5 words before and after each word for context\n",
    "   - `min_count=1`: Include words that appear at least once\n",
    "   - `workers=3`: Use 3 CPU cores for parallel training\n",
    "\n",
    "3. **Generate embeddings for each chunk**:\n",
    "   - Get the vector for each word in the chunk from the trained model\n",
    "   - Average all word vectors to create a single chunk vector\n",
    "   - If a word isn't in the model's vocabulary, use a zero vector\n",
    "\n",
    "### Key Insight:\n",
    "**Word2Vec creates chunk embeddings by averaging the individual word vectors.** This gives us a fixed-size representation (100 dimensions) for each chunk, regardless of chunk length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 240 Word2Vec chunk embeddings.\n",
      "Generated Word2Vec first chunk embedding dimension (100,).\n"
     ]
    }
   ],
   "source": [
    "# Define a function to train Word2Vec on the given chunks and returns vector averages for each chunk.\n",
    "def word2vec_embedding(chunks):\n",
    "    \n",
    "    # Tokenize each chunk into words\n",
    "    tokenized = [chunk.split() for chunk in chunks]\n",
    "    \n",
    "    # Train a Word2Vec model\n",
    "    model = Word2Vec(sentences=tokenized, vector_size=100, window=5, min_count=1, workers=3)\n",
    "    \n",
    "    embeddings = []\n",
    "    for words in tokenized:\n",
    "        vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "        # Take average vector for each chunk\n",
    "        chunk_vector = np.mean(vectors, axis=0) if vectors else np.zeros(100)\n",
    "        embeddings.append(chunk_vector)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Run Word2Vec embeddings\n",
    "w2v_embeddings = word2vec_embedding(text_chunks)\n",
    "print(f\"Generated {len(w2v_embeddings)} Word2Vec chunk embeddings.\")\n",
    "print(f\"Generated Word2Vec first chunk embedding dimension {w2v_embeddings[0].shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect a Word2Vec Embedding\n",
    "\n",
    "Let's look at the first 10 values of the first chunk's embedding vector to see what Word2Vec produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00612958,  0.00719691,  0.00240686, -0.00054739, -0.0003562 ,\n",
       "       -0.01261691,  0.00319221,  0.01749715, -0.00551196, -0.00324367],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_embeddings[0][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Observation: Word2Vec Embeddings\n",
    "\n",
    "Let's analyze what we just accomplished:\n",
    "\n",
    "### What happened:\n",
    "- The `word2vec_embedding()` function **tokenized our text into 251 chunks** and **trained a Word2Vec model** on them\n",
    "- It generated a **fixed-size vector (100 dimensions)** for each word, then **averaged them** to represent the entire chunk\n",
    "- This method works well for capturing **word-level relationships** like similarity and analogies\n",
    "\n",
    "### Limitations:\n",
    "- ‚ö†Ô∏è **Does not consider sentence structure or word order**: \"dog bites man\" and \"man bites dog\" would have very similar embeddings\n",
    "- ‚ö†Ô∏è **May miss deeper meaning or context**: It focuses on word proximity, not semantic understanding\n",
    "\n",
    "---\n",
    "\n",
    "## Good for:\n",
    "‚úÖ **Quick, lightweight embedding generation**  \n",
    "‚úÖ **Projects focused on word-level understanding** (e.g., finding synonyms, related terms)\n",
    "\n",
    "## Limitations:\n",
    "‚ùå **Not ideal for understanding full sentences or complex context**  \n",
    "‚ùå **Accuracy depends heavily on the size and quality of training data**\n",
    "\n",
    "For production RAG systems where **semantic understanding matters**, we typically use more advanced models like OpenAI's embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Approach 2: OpenAI using Langchain Embeddings\n",
    "\n",
    "Now let's use OpenAI's powerful pre-trained embedding model. This approach provides:\n",
    "- **Higher quality embeddings** trained on massive datasets\n",
    "- **Better semantic understanding** of context and meaning\n",
    "- **1536-dimensional vectors** that capture rich information\n",
    "\n",
    "OpenAI provides powerful pre-trained models for creating embeddings using their API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load and Chunk Using RecursiveCharacterTextSplitter\n",
    "\n",
    "Unlike sentence-based chunking for Word2Vec, we'll use **RecursiveCharacterTextSplitter** here. This is a smarter chunking strategy:\n",
    "\n",
    "### Why RecursiveCharacterTextSplitter?\n",
    "- **Preserves context better**: Tries to keep related information together\n",
    "- **Configurable overlap**: Ensures no information is lost at chunk boundaries\n",
    "- **Optimized for semantic models**: Works better with advanced embedding models like OpenAI\n",
    "\n",
    "### Function parameters:\n",
    "- `chunk_size=500`: Each chunk will be approximately 500 characters\n",
    "- `chunk_overlap=50`: Chunks will overlap by 50 characters to preserve context\n",
    "\n",
    "### How it works:\n",
    "1. Load the PDF using PyMuPDFLoader\n",
    "2. Create a RecursiveCharacterTextSplitter with specified parameters\n",
    "3. Split documents into chunks with overlap\n",
    "4. Extract just the text content (page_content) from each chunk\n",
    "5. Return a list of text strings ready for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 135 OpenAI embeddings.\n",
      "First chunk embedding size: 1536\n",
      "First chunk embedding : [-0.01231426652520895, -0.017526544630527496, 0.012570381164550781, -0.02590218558907509, -0.014536234550178051, 0.031730521470308304, -0.004132443573325872, -0.017277352511882782, -0.00802953913807869, -0.01603139005601406]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and chunk using RecursiveCharacterTextSplitter\n",
    "def get_recursive_chunks(pdf_path, chunk_size=500, chunk_overlap=50):\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    raw_docs = loader.load()\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = splitter.split_documents(raw_docs)\n",
    "    \n",
    "    # Extract just the text part for embedding\n",
    "    return [chunk.page_content for chunk in chunks]\n",
    "\n",
    "# Example: Running it all together\n",
    "pdf_path = \"./data/41598_2020_Article_64454.pdf\"  # Update this if needed\n",
    "text_chunks = get_recursive_chunks(pdf_path)\n",
    "openai_embeddings = get_embeddings(text_chunks)\n",
    "\n",
    "print(f\"Generated {len(openai_embeddings)} OpenAI embeddings.\")\n",
    "print(f\"First chunk embedding size: {len(openai_embeddings[0])}\")\n",
    "print(f\"First chunk embedding : {openai_embeddings[0][0:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Observation: OpenAI Embeddings (text-embedding-ada-002)\n",
    "\n",
    "Let's analyze what makes OpenAI embeddings superior for production use:\n",
    "\n",
    "### What we did:\n",
    "- We used the `get_embeddings()` function to generate vector embeddings using **LangChain's OpenAIEmbeddings** wrapper\n",
    "- For each chunk of text, the function called OpenAI's **text-embedding-ada-002** model and returned a **1536-dimensional vector**\n",
    "- These embeddings are **context-rich** and designed for tasks like:\n",
    "  - Semantic search\n",
    "  - Question answering\n",
    "  - Document retrieval\n",
    "\n",
    "---\n",
    "\n",
    "## What this means:\n",
    "\n",
    "### üéØ Deep Semantic Understanding\n",
    "OpenAI embeddings capture the **deep meaning of entire text chunks** ‚Äî not just word proximity or surface-level patterns. The model understands:\n",
    "- **Intent**: What the text is trying to convey\n",
    "- **Topic**: The subject matter and domain\n",
    "- **Semantic structure**: How concepts relate to each other\n",
    "\n",
    "### üöÄ Optimized for Retrieval\n",
    "These embeddings are specifically optimized for **retrieval use cases**, making them an excellent choice for building robust RAG pipelines.\n",
    "\n",
    "### üìä High-Dimensional Representation\n",
    "The **1536-dimensional vector** holds rich information about the intent, topic, and semantic structure of the input text. This density of information enables:\n",
    "- More accurate similarity matching\n",
    "- Better handling of nuanced queries\n",
    "- Improved retrieval of relevant context\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison: Word2Vec vs OpenAI Embeddings\n",
    "\n",
    "| Aspect | Word2Vec | OpenAI (text-embedding-ada-002) |\n",
    "|--------|----------|----------------------------------|\n",
    "| **Dimension Size** | Custom (e.g., 100) | Fixed (1536) |\n",
    "| **Training** | Trained on your data | Pre-trained on massive datasets |\n",
    "| **Context Understanding** | Word proximity only | Deep semantic meaning |\n",
    "| **Best For** | Word-level relationships | Semantic search, RAG, QA |\n",
    "| **Production Ready** | Requires careful tuning | Ready to use out-of-the-box |\n",
    "| **Quality** | Depends on training data | Consistently high quality |\n",
    "\n",
    "### üí° Key Takeaway:\n",
    "For production RAG systems, **OpenAI embeddings are the preferred choice** due to their superior semantic understanding, consistency, and optimization for retrieval tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Next Steps\n",
    "\n",
    "In this tutorial, we've covered:\n",
    "\n",
    "### ‚úÖ What We Learned:\n",
    "1. **What embeddings are** and why they're critical for RAG systems\n",
    "2. **Two embedding approaches**:\n",
    "   - **Word2Vec**: Fast, lightweight, word-level understanding\n",
    "   - **OpenAI (text-embedding-ada-002)**: Deep semantic understanding, production-ready\n",
    "3. **Different chunking strategies**:\n",
    "   - Sentence-based chunking for Word2Vec\n",
    "   - RecursiveCharacterTextSplitter for OpenAI embeddings\n",
    "4. **How to generate and compare embeddings** from both methods\n",
    "\n",
    "### üéØ Key Insights:\n",
    "- **Word2Vec** is good for quick prototypes and word-level analysis\n",
    "- **OpenAI embeddings** are superior for production RAG systems\n",
    "- **Chunking strategy matters**: Match your chunking approach to your embedding method\n",
    "- **1536 dimensions** provide much richer semantic information than 100\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. **Store embeddings in a vector database** (e.g., Pinecone, Weaviate, FAISS)\n",
    "2. **Implement similarity search** to find relevant chunks for user queries\n",
    "3. **Build a complete RAG pipeline** that:\n",
    "   - Takes user questions\n",
    "   - Converts questions to embeddings\n",
    "   - Finds similar document chunks\n",
    "   - Generates answers using retrieved context\n",
    "4. **Experiment with different chunk sizes** to optimize retrieval quality\n",
    "5. **Add metadata** to chunks (page numbers, section titles) for better context\n",
    "\n",
    "### üìö Additional Resources:\n",
    "- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)\n",
    "- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n",
    "- [Word2Vec Tutorial](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "- [RAG Best Practices](https://www.pinecone.io/learn/retrieval-augmented-generation/)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now understand how to transform documents into embeddings for RAG systems. üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
