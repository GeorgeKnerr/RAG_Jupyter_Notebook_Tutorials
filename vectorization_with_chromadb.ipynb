{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChromaDB Tutorial: Storing & Retrieving Vectorized Data\n",
    "\n",
    "This notebook demonstrates how to use ChromaDB for vector storage and retrieval with support for both OpenAI API and local Ollama models.\n",
    "\n",
    "## Overview\n",
    "- Load and process PDF documents\n",
    "- Create embeddings using OpenAI or local models (Ollama)\n",
    "- Store embeddings in ChromaDB\n",
    "- Perform semantic search\n",
    "- Implement hybrid retrieval (BM25 + semantic)\n",
    "- Re-rank results using LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Imports\n",
    "\n",
    "First, install required packages:\n",
    "```bash\n",
    "pip install chromadb langchain langchain-community langchain-openai pypdf rank-bm25 sentence-transformers openai ollama\n",
    "```\n",
    "\n",
    "Import all necessary libraries for document processing, embeddings, and vector storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import warnings\n",
    "from typing import List\n",
    "\n",
    "# Document processing\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.schema import Document\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "# Embeddings - OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Embeddings - Local (sentence-transformers)\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# LLM clients\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "\n",
    "# Utilities\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set up configuration for either OpenAI or Ollama:\n",
    "\n",
    "**For OpenAI:**\n",
    "- Set the `OPENAI_API_KEY` environment variable\n",
    "- Uses OpenAI's `text-embedding-3-small` for embeddings\n",
    "- Uses GPT-4 for re-ranking\n",
    "\n",
    "**For Ollama (Local):**\n",
    "- Requires Ollama to be installed and running locally\n",
    "- Uses sentence-transformers for embeddings (e.g., `all-MiniLM-L6-v2`)\n",
    "- Uses any Ollama model for chat (e.g., `llama2`, `mistral`)\n",
    "\n",
    "Choose your preferred model provider by setting `USE_OLLAMA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION: Choose your model provider\n",
    "# ============================================\n",
    "\n",
    "# Set to True for local Ollama, False for OpenAI API\n",
    "USE_OLLAMA = False  # Change to True to use Ollama\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    # Ollama Configuration (Local)\n",
    "    OLLAMA_MODEL = \"llama2\"  # or \"mistral\", \"llama3\", etc.\n",
    "    EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  # Local sentence-transformer model\n",
    "    print(f\"Using Ollama with model: {OLLAMA_MODEL}\")\n",
    "    print(f\"Using local embeddings: {EMBEDDING_MODEL}\")\n",
    "else:\n",
    "    # OpenAI Configuration\n",
    "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise ValueError(\"OPENAI_API_KEY environment variable not set!\")\n",
    "    \n",
    "    OPENAI_CHAT_MODEL = \"gpt-4\"  # or \"gpt-3.5-turbo\"\n",
    "    OPENAI_EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "    print(f\"Using OpenAI with chat model: {OPENAI_CHAT_MODEL}\")\n",
    "    print(f\"Using OpenAI embeddings: {OPENAI_EMBEDDING_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Clients and Embeddings\n",
    "\n",
    "Create the appropriate clients based on the selected configuration:\n",
    "- **Embeddings client**: Either OpenAI or local HuggingFace model\n",
    "- **Chat client**: Either OpenAI or Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings\n",
    "if USE_OLLAMA:\n",
    "    # Use local sentence-transformers model\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL,\n",
    "        model_kwargs={'device': 'cpu'},  # Use 'cuda' if GPU available\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    chat_client = None  # Ollama doesn't need a client object\n",
    "else:\n",
    "    # Use OpenAI embeddings\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=OPENAI_EMBEDDING_MODEL,\n",
    "        openai_api_key=OPENAI_API_KEY\n",
    "    )\n",
    "    chat_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(\"✓ Embeddings and chat clients initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Function: Get LLM Response\n",
    "\n",
    "This function sends a prompt to the LLM and returns the response. It automatically uses either OpenAI or Ollama based on the configuration.\n",
    "\n",
    "The `@retry` decorator ensures robustness by automatically retrying failed API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=45, max=120), stop=stop_after_attempt(6))\n",
    "def get_response(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Send a prompt to the LLM and get a response.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The user's question or instruction\n",
    "        \n",
    "    Returns:\n",
    "        The model's response as a string\n",
    "    \"\"\"\n",
    "    if USE_OLLAMA:\n",
    "        # Use Ollama\n",
    "        response = ollama.chat(\n",
    "            model=OLLAMA_MODEL,\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }]\n",
    "        )\n",
    "        return response['message']['content']\n",
    "    else:\n",
    "        # Use OpenAI\n",
    "        response = chat_client.chat.completions.create(\n",
    "            model=OPENAI_CHAT_MODEL,\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }]\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load PDF Document\n",
    "\n",
    "Use LangChain's `PyMuPDFLoader` to load a PDF file and extract text content. The loader automatically handles:\n",
    "- Text extraction from each page\n",
    "- Metadata preservation (page numbers, etc.)\n",
    "- Document structure parsing\n",
    "\n",
    "**Note:** Update the `pdf_path` variable with your actual PDF file location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_with_langchain(pdf_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load a PDF file and return LangChain Document objects.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        List of Document objects with page content and metadata\n",
    "    \"\"\"\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"Successfully loaded {len(documents)} document chunks from the PDF.\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your PDF file (update this with your actual file path)\n",
    "pdf_path = \"./Data/Healthcare doc for RAG.pdf\"\n",
    "\n",
    "# Load the document\n",
    "docs = load_pdf_with_langchain(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Split Text into Chunks\n",
    "\n",
    "For effective vector search, we split the document into smaller chunks:\n",
    "\n",
    "- **chunk_size=600**: Each chunk contains approximately 600 characters\n",
    "- **chunk_overlap=100**: Adjacent chunks share 100 characters to maintain context\n",
    "\n",
    "The `RecursiveCharacterTextSplitter` intelligently splits text at natural boundaries (paragraphs, sentences) rather than arbitrary character positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "texts = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split into {len(texts)} text chunks\")\n",
    "print(f\"\\nExample chunk:\\n{texts[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create and Persist ChromaDB Vector Store\n",
    "\n",
    "Now we'll create a ChromaDB vector store to store our document embeddings:\n",
    "\n",
    "1. **Embedding Generation**: Convert each text chunk into a vector embedding\n",
    "2. **Storage**: Store embeddings and metadata in ChromaDB\n",
    "3. **Persistence**: Save to disk for future use\n",
    "\n",
    "ChromaDB is used because:\n",
    "- Lightweight and runs locally\n",
    "- Fast similarity search\n",
    "- Easy to persist and reload\n",
    "- No external dependencies\n",
    "\n",
    "**Note:** We disable telemetry for privacy reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up tiktoken cache directory (for OpenAI)\n",
    "tiktoken_cache_dir = os.path.abspath(\"./.setup/tiktoken_cache/\")\n",
    "os.environ[\"TIKTOKEN_CACHE_DIR\"] = tiktoken_cache_dir\n",
    "\n",
    "# Disable ChromaDB telemetry for privacy\n",
    "os.environ[\"ANONYMIZED_TELEMETRY\"] = \"False\"\n",
    "\n",
    "# Define persist directory\n",
    "persist_directory = './vector_embeddings_OPENAI' if not USE_OLLAMA else './vector_embeddings_LOCAL'\n",
    "\n",
    "# Create ChromaDB vector store\n",
    "try:\n",
    "    # Clean up existing database if needed\n",
    "    if os.path.exists(persist_directory):\n",
    "        shutil.rmtree(persist_directory)\n",
    "    \n",
    "    # Create new vector store\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=texts,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    \n",
    "    # Persist to disk\n",
    "    vectordb.persist()\n",
    "    \n",
    "    print(f\"✓ Embeddings stored in ChromaDB at: {persist_directory}\")\n",
    "    print(f\"✓ Vector store contains {vectordb._collection.count()} embeddings\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Creating a new ChromaDB database.\")\n",
    "    \n",
    "    # Generate random directory name\n",
    "    persist_directory = f\"{persist_directory}_{(''.join(str(random.randint(0, 9)) for _ in range(4)))}\"\n",
    "    \n",
    "    if os.path.exists(persist_directory):\n",
    "        shutil.rmtree(persist_directory)\n",
    "    \n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=texts,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    \n",
    "    print(f\"✓ New database created at: {persist_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Semantic Retrieval Function\n",
    "\n",
    "This function performs semantic search to find the most relevant document chunks:\n",
    "\n",
    "1. **Query Embedding**: Convert the search query into a vector\n",
    "2. **Similarity Search**: Find chunks with similar embeddings using cosine similarity\n",
    "3. **Deduplication**: Remove duplicate results\n",
    "4. **Top-K Selection**: Return only the most relevant results\n",
    "\n",
    "The function fetches more results than needed (`k=top_k*2`) to ensure we have enough unique results after deduplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_retrieval(query: str, top_k: int = 3) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Retrieve top_k semantically relevant documents from ChromaDB using vector search.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        top_k: Number of documents to return\n",
    "        \n",
    "    Returns:\n",
    "        List of the most relevant Document objects\n",
    "    \"\"\"\n",
    "    # Fetch more results to ensure enough unique results after deduplication\n",
    "    results = vectordb.similarity_search(query, k=top_k*2)\n",
    "    \n",
    "    # Deduplicate based on page content\n",
    "    unique_results = []\n",
    "    seen_contents = set()\n",
    "    \n",
    "    for doc in results:\n",
    "        if doc.page_content not in seen_contents:\n",
    "            unique_results.append(doc)\n",
    "            seen_contents.add(doc.page_content)\n",
    "        \n",
    "        if len(unique_results) >= top_k:\n",
    "            break\n",
    "    \n",
    "    return unique_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic retrieval\n",
    "query = \"How does the Integrated Clinical Environment (ICE) platform support MIoT implementation in healthcare settings?\"\n",
    "semantic_results = semantic_retrieval(query)\n",
    "\n",
    "print(f\"Found {len(semantic_results)} relevant chunks\\n\")\n",
    "for i, doc in enumerate(semantic_results, 1):\n",
    "    print(f\"Semantic Result {i}:\")\n",
    "    print(f\"{doc.page_content[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hybrid Retrieval: BM25 + Semantic Search\n",
    "\n",
    "Hybrid retrieval combines two complementary approaches:\n",
    "\n",
    "**1. Semantic Search (Vector-based)**\n",
    "   - Understands meaning and context\n",
    "   - Good for conceptual queries\n",
    "   - Uses embeddings and cosine similarity\n",
    "\n",
    "**2. BM25 (Keyword-based)**\n",
    "   - Exact keyword matching\n",
    "   - Good for specific terms\n",
    "   - Uses term frequency and document frequency\n",
    "\n",
    "The function:\n",
    "1. Gets semantic search results (first half of top_k)\n",
    "2. Gets BM25 keyword results\n",
    "3. Combines unique results from both\n",
    "4. Fills remaining spots with semantic results if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_retrieval_simple(query: str, top_k: int = 3) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Combine semantic and keyword search results for diverse retrieval.\n",
    "    \n",
    "    This approach ensures we get results from both semantic similarity\n",
    "    and exact keyword matching, providing more comprehensive coverage.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        top_k: Total number of documents to return\n",
    "        \n",
    "    Returns:\n",
    "        Combined list of unique, relevant documents\n",
    "    \"\"\"\n",
    "    # Get semantic search results\n",
    "    semantic_results = vectordb.similarity_search(query, k=top_k*2)\n",
    "    semantic_contents = [doc.page_content for doc in semantic_results]\n",
    "    \n",
    "    # Get keyword search results using BM25\n",
    "    documents = [Document(page_content=doc) if isinstance(doc, str) else doc \n",
    "                 for doc in vectordb.get()[\"documents\"]]\n",
    "    bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "    keyword_results = bm25_retriever.get_relevant_documents(query, k=top_k*2)\n",
    "    \n",
    "    # Take half from semantic results\n",
    "    final_results = semantic_results[:top_k//2]\n",
    "    \n",
    "    # Add unique keyword results\n",
    "    for doc in keyword_results:\n",
    "        if len(final_results) >= top_k:\n",
    "            break\n",
    "        if doc.page_content not in semantic_contents:\n",
    "            final_results.append(doc)\n",
    "    \n",
    "    # Fill remaining spots with semantic results if needed\n",
    "    remaining_spots = top_k - len(final_results)\n",
    "    if remaining_spots > 0:\n",
    "        start_idx = len(final_results) - remaining_spots\n",
    "        final_results.extend(semantic_results[start_idx:start_idx+remaining_spots])\n",
    "    \n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hybrid retrieval\n",
    "hybrid_results = hybrid_retrieval_simple(\n",
    "    \"How does the Integrated Clinical Environment (ICE) platform support MIoT implementation in healthcare settings?\"\n",
    ")\n",
    "\n",
    "print(f\"Found {len(hybrid_results)} relevant chunks using hybrid retrieval\\n\")\n",
    "for i, doc in enumerate(hybrid_results, 1):\n",
    "    print(f\"Hybrid Result {i}:\")\n",
    "    print(f\"{doc.page_content[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. LLM-Based Re-ranking\n",
    "\n",
    "After initial retrieval, we use the LLM to re-rank results based on true relevance:\n",
    "\n",
    "**Why Re-rank?**\n",
    "- Initial retrieval uses only similarity/keywords\n",
    "- LLM can understand deeper semantic relevance\n",
    "- Can handle complex multi-aspect queries\n",
    "\n",
    "**How it works:**\n",
    "1. Send all retrieved chunks to the LLM\n",
    "2. Ask LLM to rank them by relevance to the query\n",
    "3. Parse the rankings from LLM output\n",
    "4. Return chunks in the new order\n",
    "\n",
    "This is particularly useful for complex queries where simple similarity isn't enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_rerank_with_openai(query: str, retrieved_docs: List[Document], top_k: int = 3) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Use the LLM to re-rank document chunks based on relevance to the query.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's input question\n",
    "        retrieved_docs: List of LangChain Document objects retrieved from ChromaDB\n",
    "        top_k: Number of top chunks to return after re-ranking\n",
    "        \n",
    "    Returns:\n",
    "        Sorted list of the most relevant chunks, based on LLM scoring\n",
    "    \"\"\"\n",
    "    # Step 1: Prepare the ranking prompt\n",
    "    prompt = f\"\"\"You are helping rank document chunks based on how well they answer this question:\\n\\nQuestion: {query}\\n\\n\"\"\"\n",
    "    prompt += \"Here are the chunks:\\n\\n\"\n",
    "    \n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        prompt += f\"Chunk {i+1}:\\n{doc.page_content.strip()}\\n\\n\"\n",
    "    \n",
    "    prompt += f\"Please rank the top {top_k} chunks in order of relevance. Respond only like this:\\nChunk 3, Chunk 1, Chunk 5\"\n",
    "    \n",
    "    # Step 2: Call LLM for re-ranking\n",
    "    llm_output = get_response(prompt)\n",
    "    print(f\"LLM Rerank Output:\\n{llm_output}\")\n",
    "    \n",
    "    # Step 3: Extract chunk numbers from the output\n",
    "    chunk_order = [\n",
    "        int(s.strip().split()[1]) - 1  # Convert to 0-based index\n",
    "        for s in llm_output.split(',')\n",
    "        if s.strip().startswith(\"Chunk\")\n",
    "    ]\n",
    "    \n",
    "    # Step 4: Return sorted chunk objects\n",
    "    reranked_docs = [\n",
    "        retrieved_docs[i] \n",
    "        for i in chunk_order \n",
    "        if i < len(retrieved_docs)\n",
    "    ]\n",
    "    \n",
    "    return reranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Run Complete Retrieval Pipeline\n",
    "\n",
    "Now we'll execute the full retrieval pipeline:\n",
    "\n",
    "1. **Hybrid Retrieval**: Get initial candidates using both semantic and keyword search\n",
    "2. **LLM Re-ranking**: Re-order results based on deep semantic understanding\n",
    "3. **Display Results**: Show the final ranked results\n",
    "\n",
    "This combines the strengths of all approaches for optimal retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query\n",
    "query = \"How does the Integrated Clinical Environment (ICE) platform support MIoT implementation in healthcare settings?\"\n",
    "\n",
    "# Step 1: Hybrid retrieval\n",
    "print(\"Step 1: Performing hybrid retrieval...\\n\")\n",
    "hybrid_results = hybrid_retrieval_simple(query, top_k=5)\n",
    "\n",
    "print(f\"Retrieved {len(hybrid_results)} initial results\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 2: LLM re-ranking\n",
    "print(\"\\nStep 2: Re-ranking with LLM...\\n\")\n",
    "reranked_results = llm_rerank_with_openai(query, hybrid_results, top_k=3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nFinal Reranked Results:\\n\")\n",
    "\n",
    "# Display the reranked chunks\n",
    "for i, doc in enumerate(reranked_results, 1):\n",
    "    print(f\"\\nReranked Chunk {i}:\")\n",
    "    print(f\"{doc.page_content[:300]}...\")  # Show first 300 characters\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Generate Final Answer with Context\n",
    "\n",
    "Finally, we can use the retrieved and re-ranked chunks to generate a comprehensive answer to the user's question.\n",
    "\n",
    "The LLM receives:\n",
    "- The original query\n",
    "- The most relevant document chunks as context\n",
    "- Instructions to answer based on the provided context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare context from reranked results\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in reranked_results])\n",
    "\n",
    "# Create final prompt\n",
    "final_prompt = f\"\"\"Based on the following context, answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Get final answer\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nGenerating final answer...\\n\")\n",
    "final_answer = get_response(final_prompt)\n",
    "\n",
    "print(\"Final Answer:\")\n",
    "print(\"=\"*80)\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated a complete RAG (Retrieval-Augmented Generation) pipeline:\n",
    "\n",
    "1. ✅ **Document Processing**: Loaded and chunked PDF documents\n",
    "2. ✅ **Vector Storage**: Created embeddings and stored in ChromaDB\n",
    "3. ✅ **Semantic Search**: Retrieved relevant chunks using vector similarity\n",
    "4. ✅ **Hybrid Retrieval**: Combined semantic and keyword-based search\n",
    "5. ✅ **LLM Re-ranking**: Used LLM to improve result relevance\n",
    "6. ✅ **Answer Generation**: Generated final answer using retrieved context\n",
    "\n",
    "### Key Advantages\n",
    "\n",
    "- **Flexible**: Supports both cloud (OpenAI) and local (Ollama) models\n",
    "- **No Vendor Lock-in**: No Azure dependencies\n",
    "- **Privacy-Friendly**: Can run completely offline with Ollama\n",
    "- **Cost-Effective**: Choose between paid API or free local models\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try different chunk sizes and overlap settings\n",
    "- Experiment with different embedding models\n",
    "- Add metadata filtering to ChromaDB queries\n",
    "- Implement conversation history for multi-turn QA\n",
    "- Add source citations to generated answers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
