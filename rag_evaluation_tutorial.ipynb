{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG (Retrieval-Augmented Generation) Evaluation Tutorial\n",
        "\n",
        "## A Comprehensive Guide to Building and Evaluating RAG Systems with DeepEval\n",
        "\n",
        "---\n",
        "\n",
        "### Overview\n",
        "\n",
        "This tutorial provides a complete walkthrough of building a RAG (Retrieval-Augmented Generation) system and evaluating it using the **DeepEval** framework. RAG systems combine the power of information retrieval with large language models to generate accurate, grounded responses based on source documents.\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "1. **PDF Document Processing**: How to load and chunk PDF documents for embedding\n",
        "2. **Vector Store Management**: Using ChromaDB to store and retrieve document embeddings\n",
        "3. **RAG Pipeline Implementation**: Building an end-to-end question-answering system\n",
        "4. **Retrieval Evaluation**: Measuring how well the system retrieves relevant information\n",
        "   - Contextual Precision\n",
        "   - Contextual Recall  \n",
        "   - Contextual Relevancy\n",
        "5. **Generator Evaluation**: Assessing the quality of generated answers\n",
        "   - Answer Relevancy\n",
        "   - Faithfulness\n",
        "   - Hallucination Check\n",
        "   - G-Eval (Custom Metrics)\n",
        "6. **Model Performance Comparison**: Tracking and comparing different LLM backends\n",
        "\n",
        "### LLM Backend Options\n",
        "\n",
        "This notebook supports two LLM backends:\n",
        "- **OpenAI API**: Cloud-based, requires API key\n",
        "- **Ollama**: Local inference, runs models on your machine\n",
        "\n",
        "You can switch between backends and compare their performance on the same evaluation tasks.\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "- Python 3.8+\n",
        "- OpenAI API key (for OpenAI backend)\n",
        "- Ollama installed locally (for Ollama backend)\n",
        "- The healthcare PDF document at `data/07.Healthcare_2016.pdf`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 1: Environment Setup and Dependencies\n",
        "\n",
        "### Installing Required Packages\n",
        "\n",
        "Before we begin, we need to install all the necessary Python packages. This includes:\n",
        "\n",
        "- **langchain**: Framework for building LLM applications\n",
        "- **langchain-openai**: OpenAI integration for LangChain\n",
        "- **langchain-community**: Community integrations including Ollama\n",
        "- **chromadb**: Vector database for storing embeddings\n",
        "- **deepeval**: Evaluation framework for LLM applications\n",
        "- **pymupdf**: PDF parsing library\n",
        "- **tenacity**: Retry logic for API calls\n",
        "- **pandas**: Data manipulation for performance tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q langchain langchain-openai langchain-community chromadb deepeval pymupdf tenacity pandas python-dotenv ollama openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing Libraries\n",
        "\n",
        "Now we import all the necessary libraries. We organize imports by functionality:\n",
        "\n",
        "1. **Standard library imports**: os, datetime for system operations and timestamps\n",
        "2. **LangChain imports**: For document loading, text splitting, and LLM interactions\n",
        "3. **ChromaDB imports**: For vector storage and retrieval\n",
        "4. **DeepEval imports**: For RAG evaluation metrics\n",
        "5. **Utility imports**: For retry logic and data handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "from typing import Optional, List, Dict, Any\n",
        "\n",
        "# Data handling\n",
        "import pandas as pd\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain.schema import Document\n",
        "\n",
        "# ChromaDB imports\n",
        "import chromadb\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# DeepEval imports\n",
        "from deepeval import evaluate\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.metrics import (\n",
        "    ContextualPrecisionMetric,\n",
        "    ContextualRecallMetric,\n",
        "    ContextualRelevancyMetric,\n",
        "    AnswerRelevancyMetric,\n",
        "    FaithfulnessMetric,\n",
        "    HallucinationMetric,\n",
        "    GEval\n",
        ")\n",
        "from deepeval.models.base_model import DeepEvalBaseLLM\n",
        "\n",
        "# Retry logic\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2: Configuration and Authentication\n",
        "\n",
        "### Setting Up LLM Backend Options\n",
        "\n",
        "This notebook supports two LLM backends:\n",
        "\n",
        "1. **OpenAI API**: Uses cloud-based models like GPT-4o-mini\n",
        "   - Requires `OPENAI_API_KEY` environment variable\n",
        "   - Higher quality but costs money per API call\n",
        "   \n",
        "2. **Ollama**: Uses locally-hosted open-source models\n",
        "   - Free to use once models are downloaded\n",
        "   - Runs entirely on your machine\n",
        "   - Supports models like Llama 3, Mistral, etc.\n",
        "\n",
        "### Configuration Class\n",
        "\n",
        "We create a configuration class to manage all settings in one place. This makes it easy to switch between backends and models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RAGConfig:\n",
        "    \"\"\"\n",
        "    Configuration class for RAG system settings.\n",
        "    \n",
        "    This class centralizes all configuration options including:\n",
        "    - LLM backend selection (OpenAI or Ollama)\n",
        "    - Model names for chat and embeddings\n",
        "    - Document processing parameters\n",
        "    - Vector store settings\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        backend: str = \"openai\",  # \"openai\" or \"ollama\"\n",
        "        chat_model: str = None,\n",
        "        embedding_model: str = None,\n",
        "        chunk_size: int = 600,\n",
        "        chunk_overlap: int = 100,\n",
        "        persist_directory: str = \"./single_pdf_rag_eval_db\",\n",
        "        pdf_path: str = \"data/07.Healthcare_2016.pdf\"\n",
        "    ):\n",
        "        self.backend = backend.lower()\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.persist_directory = persist_directory\n",
        "        self.pdf_path = pdf_path\n",
        "        \n",
        "        # Set default models based on backend\n",
        "        if self.backend == \"openai\":\n",
        "            self.chat_model = chat_model or \"gpt-4o-mini\"\n",
        "            self.embedding_model = embedding_model or \"text-embedding-3-small\"\n",
        "        elif self.backend == \"ollama\":\n",
        "            self.chat_model = chat_model or \"llama3.2\"\n",
        "            self.embedding_model = embedding_model or \"nomic-embed-text\"\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown backend: {backend}. Use 'openai' or 'ollama'\")\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return (\n",
        "            f\"RAGConfig(backend='{self.backend}', \"\n",
        "            f\"chat_model='{self.chat_model}', \"\n",
        "            f\"embedding_model='{self.embedding_model}')\"\n",
        "        )\n",
        "\n",
        "\n",
        "# Display available configuration options\n",
        "print(\"Available Backends:\")\n",
        "print(\"  - 'openai': Uses OpenAI API (requires OPENAI_API_KEY)\")\n",
        "print(\"  - 'ollama': Uses local Ollama models\")\n",
        "print(\"\\nDefault Models:\")\n",
        "print(\"  OpenAI: gpt-4o-mini (chat), text-embedding-3-small (embeddings)\")\n",
        "print(\"  Ollama: llama3.2 (chat), nomic-embed-text (embeddings)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize Configuration\n",
        "\n",
        "Choose your backend and model configuration here. You can run the notebook multiple times with different configurations to compare performance.\n",
        "\n",
        "**Important**: \n",
        "- For OpenAI, ensure `OPENAI_API_KEY` is set in your environment\n",
        "- For Ollama, ensure Ollama is running and models are downloaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION - MODIFY THIS CELL TO CHANGE SETTINGS\n",
        "# ============================================================\n",
        "\n",
        "# Choose backend: \"openai\" or \"ollama\"\n",
        "BACKEND = \"openai\"\n",
        "\n",
        "# Optional: Specify custom models (or leave as None for defaults)\n",
        "CHAT_MODEL = None  # e.g., \"gpt-4o\", \"llama3.2\", \"mistral\"\n",
        "EMBEDDING_MODEL = None  # e.g., \"text-embedding-3-large\", \"nomic-embed-text\"\n",
        "\n",
        "# Document settings\n",
        "PDF_PATH = \"data/07.Healthcare_2016.pdf\"\n",
        "PERSIST_DIRECTORY = \"./single_pdf_rag_eval_db\"\n",
        "\n",
        "# Create configuration\n",
        "config = RAGConfig(\n",
        "    backend=BACKEND,\n",
        "    chat_model=CHAT_MODEL,\n",
        "    embedding_model=EMBEDDING_MODEL,\n",
        "    pdf_path=PDF_PATH,\n",
        "    persist_directory=PERSIST_DIRECTORY\n",
        ")\n",
        "\n",
        "print(f\"Configuration initialized: {config}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Environment Variables and Initialize Clients\n",
        "\n",
        "Now we set up the LLM and embedding clients based on the selected backend. The code automatically handles:\n",
        "\n",
        "1. **OpenAI Backend**:\n",
        "   - Loads `OPENAI_API_KEY` from environment variables\n",
        "   - Creates `ChatOpenAI` and `OpenAIEmbeddings` clients\n",
        "\n",
        "2. **Ollama Backend**:\n",
        "   - Connects to local Ollama server (default: http://localhost:11434)\n",
        "   - Creates `ChatOllama` and `OllamaEmbeddings` clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_clients(config: RAGConfig):\n",
        "    \"\"\"\n",
        "    Initialize LLM and embedding clients based on the configuration.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    config : RAGConfig\n",
        "        Configuration object specifying backend and model settings\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    tuple : (chat_model, embeddings_model)\n",
        "        Initialized chat and embeddings clients\n",
        "    \"\"\"\n",
        "    \n",
        "    if config.backend == \"openai\":\n",
        "        # Get OpenAI API key from environment\n",
        "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise ValueError(\n",
        "                \"OPENAI_API_KEY not found in environment variables. \"\n",
        "                \"Please set it using: export OPENAI_API_KEY='your-key-here'\"\n",
        "            )\n",
        "        \n",
        "        # Initialize OpenAI clients\n",
        "        chat_model = ChatOpenAI(\n",
        "            model=config.chat_model,\n",
        "            temperature=0,\n",
        "            api_key=api_key\n",
        "        )\n",
        "        \n",
        "        embeddings_model = OpenAIEmbeddings(\n",
        "            model=config.embedding_model,\n",
        "            api_key=api_key\n",
        "        )\n",
        "        \n",
        "        print(f\"\u2713 OpenAI clients initialized\")\n",
        "        print(f\"  Chat model: {config.chat_model}\")\n",
        "        print(f\"  Embeddings model: {config.embedding_model}\")\n",
        "        \n",
        "    elif config.backend == \"ollama\":\n",
        "        # Initialize Ollama clients\n",
        "        chat_model = ChatOllama(\n",
        "            model=config.chat_model,\n",
        "            temperature=0\n",
        "        )\n",
        "        \n",
        "        embeddings_model = OllamaEmbeddings(\n",
        "            model=config.embedding_model\n",
        "        )\n",
        "        \n",
        "        print(f\"\u2713 Ollama clients initialized\")\n",
        "        print(f\"  Chat model: {config.chat_model}\")\n",
        "        print(f\"  Embeddings model: {config.embedding_model}\")\n",
        "        print(f\"  Note: Ensure Ollama is running (ollama serve)\")\n",
        "    \n",
        "    return chat_model, embeddings_model\n",
        "\n",
        "\n",
        "# Initialize clients\n",
        "chat_model, embeddings_model = initialize_clients(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 3: DeepEval Model Wrapper\n",
        "\n",
        "### Why We Need a Wrapper\n",
        "\n",
        "**Important Note**: DeepEval doesn't directly support LangChain's chat model classes. To use our LLM models with DeepEval's evaluation metrics, we need to create a wrapper class that bridges the gap between LangChain and DeepEval.\n",
        "\n",
        "### Understanding the Wrapper Class\n",
        "\n",
        "The wrapper class (`CustomModelWrapper`) implements DeepEval's `DeepEvalBaseLLM` interface. Let's walk through each method:\n",
        "\n",
        "1. **`__init__`**: The constructor takes a LangChain chat model and stores it for use in other methods.\n",
        "\n",
        "2. **`load_model`**: Returns the stored model. DeepEval may call this internally.\n",
        "\n",
        "3. **`generate`**: Takes a prompt string, calls the model's `invoke` method to send the prompt to the LLM, and returns the generated response content.\n",
        "\n",
        "4. **`a_generate`**: Similar to `generate` but asynchronous. It awaits the response and extracts the content.\n",
        "\n",
        "5. **`get_model_name`**: Returns a string identifier naming the model for DeepEval reporting and tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomModelWrapper(DeepEvalBaseLLM):\n",
        "    \"\"\"\n",
        "    Wrapper class to make LangChain chat models compatible with DeepEval.\n",
        "    \n",
        "    This wrapper bridges the gap between LangChain's ChatModel interface\n",
        "    and DeepEval's expected model interface. It enables us to use any\n",
        "    LangChain-compatible model (OpenAI, Ollama, etc.) with DeepEval's\n",
        "    evaluation metrics.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : ChatModel\n",
        "        A LangChain chat model (ChatOpenAI, ChatOllama, etc.)\n",
        "    model_name : str\n",
        "        A descriptive name for the model (used in reporting)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model, model_name: str):\n",
        "        \"\"\"\n",
        "        Initialize the wrapper with a LangChain chat model.\n",
        "        \n",
        "        The model is stored in self.model for use in other methods.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self._model_name = model_name\n",
        "    \n",
        "    def load_model(self):\n",
        "        \"\"\"\n",
        "        Return the stored model.\n",
        "        \n",
        "        DeepEval may call this method internally to access the underlying model.\n",
        "        \"\"\"\n",
        "        return self.model\n",
        "    \n",
        "    def generate(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate a response for the given prompt (synchronous).\n",
        "        \n",
        "        This method:\n",
        "        1. Calls self.model.invoke(prompt) to send the prompt to the LLM\n",
        "        2. Extracts and returns the response content\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        prompt : str\n",
        "            The input prompt to send to the model\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        str : The generated response text\n",
        "        \"\"\"\n",
        "        response = self.model.invoke(prompt)\n",
        "        return response.content\n",
        "    \n",
        "    async def a_generate(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate a response for the given prompt (asynchronous).\n",
        "        \n",
        "        Similar to generate() but asynchronous - it awaits the response\n",
        "        and extracts the content.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        prompt : str\n",
        "            The input prompt to send to the model\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        str : The generated response text\n",
        "        \"\"\"\n",
        "        response = await self.model.ainvoke(prompt)\n",
        "        return response.content\n",
        "    \n",
        "    def get_model_name(self) -> str:\n",
        "        \"\"\"\n",
        "        Return the model identifier for DeepEval reporting and tracking.\n",
        "        \n",
        "        This name appears in evaluation reports and helps identify\n",
        "        which model was used for each evaluation run.\n",
        "        \"\"\"\n",
        "        return self._model_name\n",
        "\n",
        "\n",
        "# Create the wrapped model for DeepEval\n",
        "# We pass our chat_model into the CustomModelWrapper class\n",
        "wrapped_model = CustomModelWrapper(\n",
        "    model=chat_model,\n",
        "    model_name=f\"{config.backend}_{config.chat_model}\"\n",
        ")\n",
        "\n",
        "print(f\"\u2713 Created wrapped model: {wrapped_model.get_model_name()}\")\n",
        "print(\"\\nThis wrapped model generates responses like the original model,\")\n",
        "print(\"but is fully compatible with DeepEval's interface.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 4: Performance Tracking System\n",
        "\n",
        "### Why Track Performance?\n",
        "\n",
        "When comparing different LLM backends (OpenAI vs Ollama) and models, it's essential to track performance metrics systematically. This allows you to:\n",
        "\n",
        "1. Compare how different models perform on the same evaluation tasks\n",
        "2. Track improvements or regressions over time\n",
        "3. Make informed decisions about which model to use in production\n",
        "\n",
        "### Performance Tracker Class\n",
        "\n",
        "We create a `PerformanceTracker` class that:\n",
        "- Stores evaluation results with timestamps and run numbers\n",
        "- Supports multiple evaluation categories (retrieval, generation)\n",
        "- Exports results to CSV and displays summary tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PerformanceTracker:\n",
        "    \"\"\"\n",
        "    Track and compare model performance across evaluation runs.\n",
        "    \n",
        "    This class maintains a log of all evaluation results, allowing\n",
        "    comparison between different models, backends, and configurations.\n",
        "    \n",
        "    Attributes:\n",
        "    -----------\n",
        "    results : list\n",
        "        List of dictionaries containing evaluation results\n",
        "    run_counter : int\n",
        "        Counter for tracking run numbers\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the performance tracker with empty results.\"\"\"\n",
        "        self.results = []\n",
        "        self.run_counter = 0\n",
        "    \n",
        "    def log_result(\n",
        "        self,\n",
        "        metric_name: str,\n",
        "        metric_category: str,  # \"retrieval\" or \"generation\"\n",
        "        score: float,\n",
        "        success: bool,\n",
        "        backend: str,\n",
        "        model_name: str,\n",
        "        test_variant: str = \"standard\",  # \"standard\", \"with_noise\", \"at_k\"\n",
        "        reason: str = None,\n",
        "        additional_info: dict = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Log an evaluation result.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        metric_name : str\n",
        "            Name of the evaluation metric\n",
        "        metric_category : str\n",
        "            Category: \"retrieval\" or \"generation\"\n",
        "        score : float\n",
        "            The evaluation score (0.0 to 1.0)\n",
        "        success : bool\n",
        "            Whether the evaluation passed the threshold\n",
        "        backend : str\n",
        "            LLM backend used (\"openai\" or \"ollama\")\n",
        "        model_name : str\n",
        "            Specific model name\n",
        "        test_variant : str\n",
        "            Variant of the test (standard, with_noise, at_k)\n",
        "        reason : str\n",
        "            Explanation of the score\n",
        "        additional_info : dict\n",
        "            Any additional metadata\n",
        "        \"\"\"\n",
        "        self.run_counter += 1\n",
        "        \n",
        "        result = {\n",
        "            \"run_number\": self.run_counter,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"metric_name\": metric_name,\n",
        "            \"metric_category\": metric_category,\n",
        "            \"test_variant\": test_variant,\n",
        "            \"score\": score,\n",
        "            \"success\": success,\n",
        "            \"backend\": backend,\n",
        "            \"model_name\": model_name,\n",
        "            \"reason\": reason\n",
        "        }\n",
        "        \n",
        "        if additional_info:\n",
        "            result.update(additional_info)\n",
        "        \n",
        "        self.results.append(result)\n",
        "        \n",
        "    def get_dataframe(self) -> pd.DataFrame:\n",
        "        \"\"\"Return results as a pandas DataFrame.\"\"\"\n",
        "        return pd.DataFrame(self.results)\n",
        "    \n",
        "    def get_summary(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Get a summary of results grouped by model and metric.\n",
        "        \n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame : Summary statistics\n",
        "        \"\"\"\n",
        "        df = self.get_dataframe()\n",
        "        if df.empty:\n",
        "            return df\n",
        "        \n",
        "        summary = df.groupby(['backend', 'model_name', 'metric_name', 'test_variant']).agg({\n",
        "            'score': ['mean', 'std', 'count'],\n",
        "            'success': 'mean'\n",
        "        }).round(3)\n",
        "        \n",
        "        return summary\n",
        "    \n",
        "    def save_to_csv(self, filepath: str = \"rag_performance_results.csv\"):\n",
        "        \"\"\"Save results to a CSV file.\"\"\"\n",
        "        df = self.get_dataframe()\n",
        "        df.to_csv(filepath, index=False)\n",
        "        print(f\"\u2713 Results saved to {filepath}\")\n",
        "    \n",
        "    def display_results(self):\n",
        "        \"\"\"Display formatted results table.\"\"\"\n",
        "        df = self.get_dataframe()\n",
        "        if df.empty:\n",
        "            print(\"No results recorded yet.\")\n",
        "            return\n",
        "        \n",
        "        # Select key columns for display\n",
        "        display_cols = [\n",
        "            'run_number', 'timestamp', 'metric_name', 'test_variant',\n",
        "            'score', 'success', 'backend', 'model_name'\n",
        "        ]\n",
        "        \n",
        "        # Format timestamp for readability\n",
        "        display_df = df[display_cols].copy()\n",
        "        display_df['timestamp'] = pd.to_datetime(display_df['timestamp']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        display_df['score'] = display_df['score'].round(3)\n",
        "        \n",
        "        return display_df\n",
        "\n",
        "\n",
        "# Initialize the global performance tracker\n",
        "performance_tracker = PerformanceTracker()\n",
        "\n",
        "print(\"\u2713 Performance Tracker initialized\")\n",
        "print(\"\\nThe tracker will log all evaluation results with:\")\n",
        "print(\"  - Run number and timestamp\")\n",
        "print(\"  - Metric name and category\")\n",
        "print(\"  - Score and success status\")\n",
        "print(\"  - Backend and model information\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 5: PDF Document Loading\n",
        "\n",
        "### Loading PDFs with LangChain\n",
        "\n",
        "The first step in our RAG pipeline is loading the source document. We use LangChain's `PyMuPDFLoader` to load PDF content. PyMuPDF is a fast and accurate PDF parsing library.\n",
        "\n",
        "### The `load_pdf_with_langchain` Function\n",
        "\n",
        "This function:\n",
        "1. Takes a PDF file path as input\n",
        "2. Uses PyMuPDFLoader to parse the PDF\n",
        "3. Returns the text content as document chunks\n",
        "\n",
        "Each document chunk includes:\n",
        "- `page_content`: The extracted text\n",
        "- `metadata`: Information about the source (file path, page number, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_pdf_with_langchain(pdf_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load a PDF file and extract its text content using LangChain's PyMuPDFLoader.\n",
        "    \n",
        "    PyMuPDF (also known as fitz) is a fast and accurate PDF parsing library\n",
        "    that extracts text while preserving document structure.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    pdf_path : str\n",
        "        Path to the PDF file to load\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    List[Document] : List of LangChain Document objects containing:\n",
        "        - page_content: The extracted text from each page\n",
        "        - metadata: Source information (file path, page number)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Verify file exists\n",
        "    if not os.path.exists(pdf_path):\n",
        "        raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
        "    \n",
        "    # Initialize the PyMuPDF loader\n",
        "    loader = PyMuPDFLoader(pdf_path)\n",
        "    \n",
        "    # Load and return documents\n",
        "    documents = loader.load()\n",
        "    \n",
        "    print(f\"\u2713 Loaded {len(documents)} pages from: {pdf_path}\")\n",
        "    \n",
        "    return documents\n",
        "\n",
        "\n",
        "# Test the function (don't run yet - we'll use it in the pipeline)\n",
        "print(\"Function 'load_pdf_with_langchain' defined.\")\n",
        "print(f\"\\nPDF path configured: {config.pdf_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 6: Document Chunking\n",
        "\n",
        "### Why Chunk Documents?\n",
        "\n",
        "Large documents need to be split into smaller chunks for several reasons:\n",
        "\n",
        "1. **Embedding Limitations**: Embedding models have token limits\n",
        "2. **Retrieval Precision**: Smaller chunks enable more precise retrieval\n",
        "3. **Context Window**: LLMs have limited context windows\n",
        "\n",
        "### RecursiveCharacterTextSplitter\n",
        "\n",
        "We use LangChain's `RecursiveCharacterTextSplitter` which:\n",
        "- Splits text at natural boundaries (paragraphs, sentences, words)\n",
        "- Maintains chunk overlap to preserve context at boundaries\n",
        "\n",
        "### Configuration\n",
        "\n",
        "- **chunk_size**: 600 characters per chunk\n",
        "- **chunk_overlap**: 100 characters overlap between consecutive chunks\n",
        "\n",
        "The overlap ensures that context is preserved across chunk boundaries, which is important when relevant information spans multiple chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chunk_documents(documents: List[Document], chunk_size: int = 600, chunk_overlap: int = 100) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Split documents into smaller chunks for embedding.\n",
        "    \n",
        "    Uses RecursiveCharacterTextSplitter which splits text at natural\n",
        "    boundaries (paragraphs, sentences, words) while maintaining overlap\n",
        "    to preserve context at chunk boundaries.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    documents : List[Document]\n",
        "        List of LangChain Document objects to split\n",
        "    chunk_size : int\n",
        "        Maximum size of each chunk in characters (default: 600)\n",
        "    chunk_overlap : int\n",
        "        Number of characters to overlap between chunks (default: 100)\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    List[Document] : List of chunked documents\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize the text splitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "    )\n",
        "    \n",
        "    # Split all documents\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    \n",
        "    print(f\"\u2713 Split into {len(chunks)} chunks\")\n",
        "    print(f\"  Chunk size: {chunk_size} chars\")\n",
        "    print(f\"  Overlap: {chunk_overlap} chars\")\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "\n",
        "print(\"Function 'chunk_documents' defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 7: ChromaDB Configuration\n",
        "\n",
        "### About ChromaDB\n",
        "\n",
        "ChromaDB is an open-source embedding database that:\n",
        "- Stores document embeddings as vectors\n",
        "- Enables fast similarity search\n",
        "- Persists data to disk for reuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure ChromaDB settings\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "os.environ[\"ANONYMIZED_TELEMETRY\"] = \"False\"\n",
        "\n",
        "print(\"\u2713 ChromaDB configuration set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 8: Vector Store Management\n",
        "\n",
        "### The `store_embeddings` Function\n",
        "\n",
        "This function manages embeddings using ChromaDB with retry logic for robustness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@retry(stop=stop_after_attempt(6), wait=wait_exponential(min=45, max=120))\n",
        "def store_embeddings(\n",
        "    persist_directory: str,\n",
        "    docs: Optional[List[Document]] = None,\n",
        "    embedding_function=None\n",
        ") -> Chroma:\n",
        "    \"\"\"\n",
        "    Manage embeddings using ChromaDB - load existing or create new.\n",
        "    \"\"\"\n",
        "    if embedding_function is None:\n",
        "        raise ValueError(\"embedding_function is required\")\n",
        "    \n",
        "    if os.path.exists(persist_directory) and os.listdir(persist_directory):\n",
        "        print(f\"Loading existing vector store from: {persist_directory}\")\n",
        "        vector_store = Chroma(\n",
        "            persist_directory=persist_directory,\n",
        "            embedding_function=embedding_function\n",
        "        )\n",
        "        print(f\"\u2713 Loaded vector store\")\n",
        "    else:\n",
        "        if docs is None:\n",
        "            raise ValueError(\"docs parameter required for first-time embedding\")\n",
        "        print(f\"Creating new vector store in: {persist_directory}\")\n",
        "        vector_store = Chroma.from_documents(\n",
        "            documents=docs,\n",
        "            embedding=embedding_function,\n",
        "            persist_directory=persist_directory\n",
        "        )\n",
        "        print(f\"\u2713 Created vector store with {len(docs)} chunks\")\n",
        "    \n",
        "    return vector_store\n",
        "\n",
        "print(\"Function 'store_embeddings' defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 9: Retrieval and Answer Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_chunks(query: str, vector_store: Chroma, top_k: int = 5) -> List[Document]:\n",
        "    \"\"\"Find the most semantically relevant chunks for a query.\"\"\"\n",
        "    results = vector_store.similarity_search(query, k=top_k)\n",
        "    \n",
        "    seen_content = set()\n",
        "    unique_results = []\n",
        "    for doc in results:\n",
        "        if doc.page_content not in seen_content:\n",
        "            seen_content.add(doc.page_content)\n",
        "            unique_results.append(doc)\n",
        "    \n",
        "    print(f\"Retrieved {len(unique_results)} unique chunks\")\n",
        "    return unique_results\n",
        "\n",
        "\n",
        "def generate_answer(query: str, chunks: List[Document], chat_model) -> str:\n",
        "    \"\"\"Generate an answer using the LLM based on retrieved chunks.\"\"\"\n",
        "    context = \"\\n\\n\".join([chunk.page_content for chunk in chunks])\n",
        "    \n",
        "    prompt = f\"\"\"Use only the context below to answer the question. \n",
        "If the answer is not found in the context, say \"This query is not as per the PDF.\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    \n",
        "    response = chat_model.invoke(prompt)\n",
        "    return response.content\n",
        "\n",
        "print(\"Functions 'retrieve_chunks' and 'generate_answer' defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 10: Complete RAG Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pdf_chatbot_pipeline(\n",
        "    pdf_path: str,\n",
        "    query: str,\n",
        "    persist_directory: str,\n",
        "    chat_model,\n",
        "    embedding_function,\n",
        "    chunk_size: int = 600,\n",
        "    chunk_overlap: int = 100,\n",
        "    top_k: int = 5\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Complete RAG pipeline for PDF-based question answering.\"\"\"\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"PDF Chatbot Pipeline\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Check if vector store exists\n",
        "    if os.path.exists(persist_directory) and os.listdir(persist_directory):\n",
        "        print(\"\\n[Step 1] Loading existing vector store...\")\n",
        "        vector_store = Chroma(\n",
        "            persist_directory=persist_directory,\n",
        "            embedding_function=embedding_function\n",
        "        )\n",
        "    else:\n",
        "        print(\"\\n[Step 1] No existing embeddings found. Processing PDF...\")\n",
        "        documents = load_pdf_with_langchain(pdf_path)\n",
        "        chunks = chunk_documents(documents, chunk_size, chunk_overlap)\n",
        "        print(f\"\\nLoaded {len(chunks)} document chunks from PDF\")\n",
        "        \n",
        "        print(f\"\\n[Step 2] Creating new vector store...\")\n",
        "        vector_store = store_embeddings(\n",
        "            persist_directory=persist_directory,\n",
        "            docs=chunks,\n",
        "            embedding_function=embedding_function\n",
        "        )\n",
        "    \n",
        "    print(\"\\n[Step 3] Retrieving relevant chunks...\")\n",
        "    retrieved_chunks = retrieve_chunks(query, vector_store, top_k)\n",
        "    \n",
        "    print(\"\\n[Step 4] Generating answer...\")\n",
        "    answer = generate_answer(query, retrieved_chunks, chat_model)\n",
        "    \n",
        "    context = [chunk.page_content for chunk in retrieved_chunks]\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Pipeline Complete\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    return {\n",
        "        'question': query,\n",
        "        'context': context,\n",
        "        'answer': answer,\n",
        "        'chunks': retrieved_chunks\n",
        "    }\n",
        "\n",
        "print(\"Function 'pdf_chatbot_pipeline' defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 11: Running the Pipeline\n",
        "\n",
        "### Test Query: How does MIoT improve hospital safety?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the test query\n",
        "test_query = \"How does MIoT improve hospital safety?\"\n",
        "\n",
        "# Run the pipeline\n",
        "response = pdf_chatbot_pipeline(\n",
        "    pdf_path=config.pdf_path,\n",
        "    query=test_query,\n",
        "    persist_directory=config.persist_directory,\n",
        "    chat_model=chat_model,\n",
        "    embedding_function=embeddings_model,\n",
        "    chunk_size=config.chunk_size,\n",
        "    chunk_overlap=config.chunk_overlap\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESPONSE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nQuestion: {response['question']}\")\n",
        "print(f\"\\nAnswer:\\n{response['answer']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store results for evaluation\n",
        "retrieved_context = response['context']\n",
        "ai_output = response['answer']\n",
        "\n",
        "print(f\"Retrieved {len(retrieved_context)} context chunks for evaluation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 12: Preparing for Evaluation\n",
        "\n",
        "### Human-Written Expected Response and Noise Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Human-written expected response\n",
        "expected_output = \"\"\"MIoT (Medical Internet of Things) improves hospital safety in several key ways:\n",
        "\n",
        "1. Error Reduction: MIoT systems help minimize preventable medical errors through real-time \n",
        "   monitoring and automated alerts.\n",
        "\n",
        "2. Real-time Monitoring: Connected medical devices provide continuous patient monitoring.\n",
        "\n",
        "3. Interoperability: The ICE standard enables cross-device communication.\n",
        "\n",
        "4. Infection Prevention: Connected systems can track hygiene compliance.\n",
        "\n",
        "5. Asset Management: IoT sensors track medical equipment location and status.\n",
        "\n",
        "6. Decision Support: Aggregated data provides better clinical decision-making.\"\"\"\n",
        "\n",
        "# Create noise chunks\n",
        "noise_chunks = [\n",
        "    \"\"\"Healthcare marketing strategies have evolved significantly in the digital age. \n",
        "    Social media platforms enable hospitals to reach potential patients through targeted \n",
        "    advertising campaigns.\"\"\",\n",
        "    \n",
        "    \"\"\"Healthcare consulting fees vary widely depending on the scope of services provided. \n",
        "    Management consultants typically charge between $200-500 per hour.\"\"\",\n",
        "    \n",
        "    \"\"\"Patient behavior research indicates that convenience is a primary factor in \n",
        "    healthcare provider selection. Studies show that 67% prefer providers within 10 miles.\"\"\",\n",
        "    \n",
        "    \"\"\"Healthcare economics examines the production and consumption of health and healthcare. \n",
        "    Supply and demand dynamics play crucial roles in determining costs.\"\"\"\n",
        "]\n",
        "\n",
        "# Create context variants\n",
        "clean_context = retrieved_context.copy()\n",
        "context_with_noise = retrieved_context.copy() + noise_chunks\n",
        "noise_at_top_context = noise_chunks + retrieved_context.copy()\n",
        "\n",
        "print(\"Context variants created:\")\n",
        "print(f\"  Clean context: {len(clean_context)} chunks\")\n",
        "print(f\"  Context with noise: {len(context_with_noise)} chunks\")\n",
        "print(f\"  Noise at top: {len(noise_at_top_context)} chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 13: Retrieval Evaluation - Contextual Precision\n",
        "\n",
        "**Contextual Precision** measures how well the retrieval system ranks relevant chunks at the top."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Contextual Precision metric\n",
        "contextual_precision_metric = ContextualPrecisionMetric(\n",
        "    threshold=0.6,\n",
        "    model=wrapped_model,\n",
        "    verbose_mode=True\n",
        ")\n",
        "\n",
        "# Test with clean context\n",
        "precision_test_clean = LLMTestCase(\n",
        "    input=test_query,\n",
        "    actual_output=ai_output,\n",
        "    expected_output=expected_output,\n",
        "    retrieval_context=clean_context\n",
        ")\n",
        "\n",
        "print(\"Running Contextual Precision evaluation (Clean Context)...\")\n",
        "precision_results = evaluate(test_cases=[precision_test_clean], metrics=[contextual_precision_metric])\n",
        "\n",
        "score = precision_results.test_results[0].metrics_data[0].score\n",
        "success = precision_results.test_results[0].metrics_data[0].success\n",
        "reason = precision_results.test_results[0].metrics_data[0].reason\n",
        "\n",
        "print(f\"\\nResults: Success={success}, Score={score}\")\n",
        "print(f\"Reason: {reason}\")\n",
        "\n",
        "performance_tracker.log_result(\n",
        "    metric_name=\"Contextual Precision\",\n",
        "    metric_category=\"retrieval\",\n",
        "    score=score, success=success,\n",
        "    backend=config.backend, model_name=config.chat_model,\n",
        "    test_variant=\"clean_context\", reason=reason\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 14: Retrieval Evaluation - Contextual Recall\n",
        "\n",
        "**Contextual Recall** measures how well the retrieved context supports the expected output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Contextual Recall metric\n",
        "contextual_recall_metric = ContextualRecallMetric(\n",
        "    threshold=0.6,\n",
        "    model=wrapped_model,\n",
        "    verbose_mode=True\n",
        ")\n",
        "\n",
        "# Test with clean context\n",
        "recall_test_clean = LLMTestCase(\n",
        "    input=test_query,\n",
        "    actual_output=ai_output,\n",
        "    expected_output=expected_output,\n",
        "    retrieval_context=clean_context\n",
        ")\n",
        "\n",
        "print(\"Running Contextual Recall evaluation (Clean Context)...\")\n",
        "recall_results = evaluate(test_cases=[recall_test_clean], metrics=[contextual_recall_metric])\n",
        "\n",
        "score = recall_results.test_results[0].metrics_data[0].score\n",
        "success = recall_results.test_results[0].metrics_data[0].success\n",
        "reason = recall_results.test_results[0].metrics_data[0].reason\n",
        "\n",
        "print(f\"\\nResults: Success={success}, Score={score}\")\n",
        "print(f\"Reason: {reason}\")\n",
        "\n",
        "performance_tracker.log_result(\n",
        "    metric_name=\"Contextual Recall\",\n",
        "    metric_category=\"retrieval\",\n",
        "    score=score, success=success,\n",
        "    backend=config.backend, model_name=config.chat_model,\n",
        "    test_variant=\"clean_context\", reason=reason\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 15: Retrieval Evaluation - Contextual Relevancy\n",
        "\n",
        "**Contextual Relevancy** measures how relevant the retrieved content is for answering the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Contextual Relevancy metric\n",
        "contextual_relevancy_metric = ContextualRelevancyMetric(\n",
        "    threshold=0.6,\n",
        "    model=wrapped_model,\n",
        "    verbose_mode=True\n",
        ")\n",
        "\n",
        "# Test with clean context\n",
        "relevancy_test_clean = LLMTestCase(\n",
        "    input=test_query,\n",
        "    actual_output=ai_output,\n",
        "    retrieval_context=clean_context\n",
        ")\n",
        "\n",
        "print(\"Running Contextual Relevancy evaluation (Clean Context)...\")\n",
        "relevancy_results = evaluate(test_cases=[relevancy_test_clean], metrics=[contextual_relevancy_metric])\n",
        "\n",
        "score = relevancy_results.test_results[0].metrics_data[0].score\n",
        "success = relevancy_results.test_results[0].metrics_data[0].success\n",
        "reason = relevancy_results.test_results[0].metrics_data[0].reason\n",
        "\n",
        "print(f\"\\nResults: Success={success}, Score={score}\")\n",
        "print(f\"Reason: {reason}\")\n",
        "\n",
        "performance_tracker.log_result(\n",
        "    metric_name=\"Contextual Relevancy\",\n",
        "    metric_category=\"retrieval\",\n",
        "    score=score, success=success,\n",
        "    backend=config.backend, model_name=config.chat_model,\n",
        "    test_variant=\"clean_context\", reason=reason\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 16: Generator Evaluation - Answer Relevancy\n",
        "\n",
        "**Answer Relevancy** evaluates how well the generated response matches the user's query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Answer Relevancy metric\n",
        "answer_relevancy_metric = AnswerRelevancyMetric(\n",
        "    threshold=0.6,\n",
        "    model=wrapped_model,\n",
        "    verbose_mode=True\n",
        ")\n",
        "\n",
        "answer_relevancy_test = LLMTestCase(\n",
        "    input=test_query,\n",
        "    actual_output=ai_output\n",
        ")\n",
        "\n",
        "print(\"Running Answer Relevancy evaluation...\")\n",
        "answer_results = evaluate(test_cases=[answer_relevancy_test], metrics=[answer_relevancy_metric])\n",
        "\n",
        "score = answer_results.test_results[0].metrics_data[0].score\n",
        "success = answer_results.test_results[0].metrics_data[0].success\n",
        "reason = answer_results.test_results[0].metrics_data[0].reason\n",
        "\n",
        "print(f\"\\nResults: Success={success}, Score={score}\")\n",
        "print(f\"Reason: {reason}\")\n",
        "\n",
        "performance_tracker.log_result(\n",
        "    metric_name=\"Answer Relevancy\",\n",
        "    metric_category=\"generation\",\n",
        "    score=score, success=success,\n",
        "    backend=config.backend, model_name=config.chat_model,\n",
        "    test_variant=\"standard\", reason=reason\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 17: Generator Evaluation - Faithfulness\n",
        "\n",
        "**Faithfulness** checks if the model's response is factually consistent with the retrieved context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Faithfulness metric\n",
        "faithfulness_metric = FaithfulnessMetric(\n",
        "    threshold=0.6,\n",
        "    model=wrapped_model,\n",
        "    verbose_mode=True\n",
        ")\n",
        "\n",
        "faithfulness_test = LLMTestCase(\n",
        "    input=test_query,\n",
        "    actual_output=ai_output,\n",
        "    retrieval_context=clean_context\n",
        ")\n",
        "\n",
        "print(\"Running Faithfulness evaluation...\")\n",
        "faithfulness_results = evaluate(test_cases=[faithfulness_test], metrics=[faithfulness_metric])\n",
        "\n",
        "score = faithfulness_results.test_results[0].metrics_data[0].score\n",
        "success = faithfulness_results.test_results[0].metrics_data[0].success\n",
        "reason = faithfulness_results.test_results[0].metrics_data[0].reason\n",
        "\n",
        "print(f\"\\nResults: Success={success}, Score={score}\")\n",
        "print(f\"Reason: {reason}\")\n",
        "\n",
        "performance_tracker.log_result(\n",
        "    metric_name=\"Faithfulness\",\n",
        "    metric_category=\"generation\",\n",
        "    score=score, success=success,\n",
        "    backend=config.backend, model_name=config.chat_model,\n",
        "    test_variant=\"standard\", reason=reason\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 18: Generator Evaluation - Hallucination Check\n",
        "\n",
        "**Hallucination** metric detects fabricated information. Lower scores are better!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Hallucination metric\n",
        "hallucination_metric = HallucinationMetric(\n",
        "    threshold=0.6,\n",
        "    model=wrapped_model,\n",
        "    verbose_mode=True\n",
        ")\n",
        "\n",
        "hallucination_test = LLMTestCase(\n",
        "    input=test_query,\n",
        "    actual_output=ai_output,\n",
        "    context=[expected_output]\n",
        ")\n",
        "\n",
        "print(\"Running Hallucination Check...\")\n",
        "print(\"Note: For hallucination, LOWER scores are BETTER!\")\n",
        "hallucination_results = evaluate(test_cases=[hallucination_test], metrics=[hallucination_metric])\n",
        "\n",
        "score = hallucination_results.test_results[0].metrics_data[0].score\n",
        "success = hallucination_results.test_results[0].metrics_data[0].success\n",
        "reason = hallucination_results.test_results[0].metrics_data[0].reason\n",
        "\n",
        "print(f\"\\nResults: Success={success}, Score={score}\")\n",
        "print(f\"Reason: {reason}\")\n",
        "\n",
        "performance_tracker.log_result(\n",
        "    metric_name=\"Hallucination\",\n",
        "    metric_category=\"generation\",\n",
        "    score=score, success=success,\n",
        "    backend=config.backend, model_name=config.chat_model,\n",
        "    test_variant=\"clean_context\", reason=reason\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 19: Generator Evaluation - G-Eval (Custom Metric)\n",
        "\n",
        "**G-Eval** allows custom evaluation criteria tailored to your use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define G-Eval metric\n",
        "geval_metric = GEval(\n",
        "    name=\"RAG Fact Checker\",\n",
        "    criteria=\"Evaluate for accuracy, completeness, grounding, and factual consistency.\",\n",
        "    evaluation_steps=[\n",
        "        \"Break the answer into individual factual statements.\",\n",
        "        \"Check if each statement is relevant to the question.\",\n",
        "        \"Compare each statement with the expected answer.\",\n",
        "        \"Verify that each statement is grounded in the context.\",\n",
        "        \"Flag any unsupported statements.\",\n",
        "        \"Calculate overall score based on accuracy and completeness.\"\n",
        "    ],\n",
        "    threshold=0.6,\n",
        "    model=wrapped_model,\n",
        "    verbose_mode=True\n",
        ")\n",
        "\n",
        "geval_test = LLMTestCase(\n",
        "    input=test_query,\n",
        "    actual_output=ai_output,\n",
        "    expected_output=expected_output,\n",
        "    retrieval_context=clean_context\n",
        ")\n",
        "\n",
        "print(\"Running G-Eval (RAG Fact Checker)...\")\n",
        "geval_results = evaluate(test_cases=[geval_test], metrics=[geval_metric])\n",
        "\n",
        "score = geval_results.test_results[0].metrics_data[0].score\n",
        "success = geval_results.test_results[0].metrics_data[0].success\n",
        "reason = geval_results.test_results[0].metrics_data[0].reason\n",
        "\n",
        "print(f\"\\nResults: Success={success}, Score={score}\")\n",
        "print(f\"Reason: {reason}\")\n",
        "\n",
        "performance_tracker.log_result(\n",
        "    metric_name=\"G-Eval (RAG Fact Checker)\",\n",
        "    metric_category=\"generation\",\n",
        "    score=score, success=success,\n",
        "    backend=config.backend, model_name=config.chat_model,\n",
        "    test_variant=\"standard\", reason=reason\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 20: Model Performance Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display all results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPLETE MODEL PERFORMANCE REPORT\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nBackend: {config.backend}\")\n",
        "print(f\"Chat Model: {config.chat_model}\")\n",
        "print(f\"Embedding Model: {config.embedding_model}\")\n",
        "print(f\"Total Evaluations: {performance_tracker.run_counter}\")\n",
        "\n",
        "results_df = performance_tracker.display_results()\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(results_df.to_string() if results_df is not None else \"No results\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results\n",
        "output_filename = f\"rag_performance_{config.backend}_{config.chat_model}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "performance_tracker.save_to_csv(output_filename)\n",
        "\n",
        "print(f\"\\n\u2713 Results saved to: {output_filename}\")\n",
        "print(\"\\nRun this notebook with different configurations to compare models!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Appendix: Deviations from Original Transcript\n",
        "\n",
        "This notebook has been modified from the original tutorial:\n",
        "\n",
        "### 1. Removed Azure Dependencies\n",
        "- **Original**: Used Azure OpenAI service with Azure-specific authentication\n",
        "- **Modified**: Uses standard OpenAI API with `OPENAI_API_KEY` environment variable\n",
        "\n",
        "### 2. Added Ollama Support\n",
        "- **Original**: Only supported Azure OpenAI\n",
        "- **Modified**: Supports both OpenAI API and local Ollama inference\n",
        "\n",
        "### 3. Added Performance Tracking\n",
        "- **Original**: Displayed results inline without tracking\n",
        "- **Modified**: Includes `PerformanceTracker` class for model comparison\n",
        "\n",
        "### 4. Simplified Wrapper Class\n",
        "- **Original**: `AzureChatModelWrapper` bridged Azure OpenAI and DeepEval\n",
        "- **Modified**: `CustomModelWrapper` works with any LangChain chat model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "**End of Tutorial**\n",
        "\n",
        "You now have the knowledge to:\n",
        "- Build production-ready RAG pipelines\n",
        "- Evaluate retrieval and generation quality\n",
        "- Compare different LLM backends\n",
        "- Track and improve model performance over time"
      ]
    }
  ]
}